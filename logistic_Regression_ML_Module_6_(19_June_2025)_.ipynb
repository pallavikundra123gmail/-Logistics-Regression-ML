{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theory Questions"
      ],
      "metadata": {
        "id": "j2ZHFn7TITKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. What is Logistic Regression, and how does it differ from Linear Regression\n",
        "\n",
        "#ans.>> Logistic Regression is a classification algorithm used to predict discrete classes (e.g., yes/no, 0/1).\n",
        "#       Linear Regression is a regression algorithm used to predict continuous outcomes."
      ],
      "metadata": {
        "id": "XAZZRcw9I2AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. What is the mathematical equation of Logistic Regression\n",
        "\n",
        "#ans.>>Logistic Regression predicts the probability\n",
        "# P(y=1∣x) = σ(z)= 1/1-(e**-z)"
      ],
      "metadata": {
        "id": "1qb6ju-vI4Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Why do we use the Sigmoid function in Logistic Regression\n",
        "\n",
        "#ans.>>the Sigmoid Function compresses any real-valued number into the range (0, 1), which allows us to interpret the output as a probability"
      ],
      "metadata": {
        "id": "94eYS6_TJ1jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What is the cost function of Logistic Regression\n",
        "\n",
        "#ans.>> We use Log Loss (Cross-Entropy Loss):\n",
        "# J(θ) = -(1/m) * Σ [ y(i) * log(ŷ(i)) + (1 - y(i)) * log(1 - ŷ(i)) ]"
      ],
      "metadata": {
        "id": "6o31ZD_aJgzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. What is Regularization in Logistic Regression? Why is it needed\n",
        "\n",
        "#ans.>> Regularization prevents overfitting by penalizing large coefficients in the model.\n",
        "#       Needed when: Too many features or noise causes the model to fit training data too closely.\n",
        "#       Adds a penalty term to the cost function."
      ],
      "metadata": {
        "id": "1szGHn2rJgv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "\n",
        "#ans.>>Ridge Regression: Ridge Regression adds L2 regularization (penalty on the square of coefficients)\n",
        "#          it shrinks coefficients, but doesn’t make them exactly zero.\n",
        "\n",
        "#      Lasso Regression :Lasso Regression adds L1 regularization (penalty on the absolute value of coefficients)\n",
        "#           it can shrink some coefficients to zero, effectively doing feature selection.\n",
        "\n",
        "#      Elastic Net Regression : Elastic Net Regression combines both L1 and L2 regularization, balancing between Ridge and Lasso,\n",
        "#            useful when there are many correlated features."
      ],
      "metadata": {
        "id": "7e7SOlkvJgsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. When should we use Elastic Net instead of Lasso or Ridge\n",
        "\n",
        "#ans.>>Use Elastic Net when:\n",
        "#      You have many features, and some are highly correlated.\n",
        "#      You want both regularization and feature selection."
      ],
      "metadata": {
        "id": "fyOmJkrMJgpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. What is the impact of the regularization parameter (λ) in Logistic Regression\n",
        "\n",
        "#ans.>> High λ: More regularization → simpler model → risk of underfitting.\n",
        "#       Low λ: Less regularization → risk of overfitting.\n",
        "#       λ is a hyperparameter and should be tuned using cross-validation."
      ],
      "metadata": {
        "id": "Y_JtVe-NJgTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. What are the key assumptions of Logistic Regression\n",
        "\n",
        "#ans.>> these are the key assumptions of logistic regression :-\n",
        "#      1.Linearity between features and log-odds (not output probability).\n",
        "#      2.No multicollinearity among independent variables.\n",
        "#      3.Large sample size.\n",
        "#      4.Independence of observations.\n",
        "#      5.Features should ideally be scaled (especially with regularization)."
      ],
      "metadata": {
        "id": "DAAn-28aJPIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10. What are some alternatives to Logistic Regression for classification tasks\n",
        "\n",
        "#ans.>> these are some alternatives to Logistic Regression for classification tasks :-\n",
        "#      1.Decision Trees\n",
        "#      2.Random Forest\n",
        "#      3.Support Vector Machines (SVM)\n",
        "#      4.k-Nearest Neighbors (KNN)\n",
        "#      5.Naive Bayes\n",
        "#      6.Gradient Boosting (XGBoost, LightGBM, etc.)\n",
        "#      7.Neural Networks"
      ],
      "metadata": {
        "id": "gP-u2lsdJPFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. What are Classification Evaluation Metrics\n",
        "\n",
        "#ans.>>Accuracy, Precision, Recall, F1-score, ROC-AUC, Confusion Matrix, Log Loss"
      ],
      "metadata": {
        "id": "C0BVp5ofJPBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. How does class imbalance affect Logistic Regression\n",
        "\n",
        "#ans.>> Model becomes biased toward majority class.\n",
        "#       Accuracy becomes misleading"
      ],
      "metadata": {
        "id": "J45CcJ7zJO-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. What is Hyperparameter Tuning in Logistic Regression\n",
        "\n",
        "#ans.>>It’s the process of selecting the best λ (regularization) and solver using:\n",
        "# 1.Grid Search, 2.Random Search, 3.Cross-Validation"
      ],
      "metadata": {
        "id": "aOEs98OaJOcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. What are different solvers in Logistic Regression?Which one should be used\n",
        "\n",
        "#ans.>> Common solvers in sklearn:\n",
        "#       1.liblinear: For small datasets, supports L1 & L2\n",
        "#       2.saga: Supports L1, L2, Elastic Net (best for large datasets)\n",
        "#       3.lbfgs: Fast, good for multiclass (default)\n",
        "#       4.newton-cg: Handles multiclass, uses Hessian"
      ],
      "metadata": {
        "id": "N9cNUrZ7JIk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. How is Logistic Regression extended for multiclass classification\n",
        "\n",
        "#ans.>> 1.One-vs-Rest (OvR): One binary classifier per class.\n",
        "#       2.Softmax (Multinomial Logistic Regression): Directly models all classes at once using softmax function."
      ],
      "metadata": {
        "id": "9OTP7Q5WJEf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. What are the advantages and disadvantages of Logistic Regression\n",
        "\n",
        "#ans.>> Advantages:\n",
        "#          1.Simple, easy to implement.\n",
        "#          2.Outputs probabilities.\n",
        "#          3.Works well with linearly separable data.\n",
        "#      Disadvantages:\n",
        "#         1.Poor performance on non-linear data.\n",
        "#         2.Assumes linearity in log-odds.\n",
        "#         3.Sensitive to outliers."
      ],
      "metadata": {
        "id": "idExVCdIJDfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. What are some use cases of Logistic Regression\n",
        "\n",
        "#ans.>> these are some use cases of Logistic Regression :-\n",
        "#         1.mail Spam Detection\n",
        "#         2.Customer Churn Prediction\n",
        "#         3.Credit Scoring\n",
        "#         4.Disease Prediction (e.g., diabetes, cancer)\n",
        "#         5.Marketing Campaign Responses"
      ],
      "metadata": {
        "id": "Wv7VX-cNKGwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. What is the difference between Softmax Regression and Logistic Regression\n",
        "\n",
        "#ans.>>Logistic Regression is used for binary classification (only 2 classes) and uses the sigmoid function.\n",
        "#      Softmax Regression is used for multiclass classification (3 or more classes) and uses the softmax function to give probabilities for each class.\n"
      ],
      "metadata": {
        "id": "iSyi4jR3KQlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification\n",
        "\n",
        "#ans.>> OvR: Trains one classifier per class; works well for unbalanced or sparse classes.\n",
        "#       Softmax: Trains one model; best for balanced, well-separated classes."
      ],
      "metadata": {
        "id": "LLmo5AaIKQh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "#ans.>>Each coefficient βⱼ tells us how much the log-odds of the outcome (like success/failure, yes/no, etc.)\n",
        "#       change when xⱼ increases by 1 unit, while keeping other variables the same.\n",
        "#       If βⱼ > 0: Increasing xⱼ makes the probability of class 1 (e.g., success) go up.\n",
        "#      If βⱼ < 0: Increasing xⱼ makes the probability of class 1 go down."
      ],
      "metadata": {
        "id": "o0RFugieKQez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Practical Questions"
      ],
      "metadata": {
        "id": "O5vrYVglKp2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "6K5HkBmwKQQG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data , columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "df = df[df[\"target\"] != 2]\n",
        "\n",
        "x = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train , x_test , y_train , y_test = train_test_split(x,y, test_size=0.20 , random_state=1)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Model Accuracy :\" , accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_muRR1YKPk7",
        "outputId": "4730232b-f5ca-43ba-b722-9f28ada20c25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "model_l1 = LogisticRegression(penalty=\"l1\" , solver=\"liblinear\" , max_iter=500)\n",
        "model_l1.fit(x_train,y_train)\n",
        "y_pred_l1 = model_l1.predict(x_test)\n",
        "print(\"Model Accuracy : \", accuracy_score(y_test,y_pred_l1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2W-RNO2NEn-",
        "outputId": "62d4dfc2-304c-42cf-ca7d-de3c64bc0266"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "model_l2= LogisticRegression(penalty=\"l2\" , solver=\"liblinear\" , max_iter=500)\n",
        "model_l2.fit(x_train,y_train)\n",
        "y_pred_l2 = model_l2.predict(x_test)\n",
        "print(\"Model Accuracy : \", accuracy_score(y_test,y_pred_l2))\n",
        "print(\"Coefficient :\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggZ-u9lKNKCM",
        "outputId": "ce1ca643-0782-40cd-f162-56efc4f0976b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy :  1.0\n",
            "Coefficient : [[ 0.46100411 -0.78836575  2.18624929  0.92865666]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "model_en = LogisticRegression(penalty=\"elasticnet\", solver=\"saga\",l1_ratio=0.2 , max_iter=500)\n",
        "model_en.fit(x_train,y_train)\n",
        "y_pred_en = model_en.predict(x_test)\n",
        "print(\"Model Accuracy : \",accuracy_score(y_test,y_pred_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzNiXw_fNNtI",
        "outputId": "e4858e25-5400-4355-9e2a-17c6486ac793"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "model_ovr = LogisticRegression(multi_class=\"ovr\" , max_iter=500)\n",
        "model_ovr.fit(x_train,y_train)\n",
        "y_pred_ovr = model_ovr.predict(x_test)\n",
        "print(\"Model Accuracy :\",accuracy_score(y_test,y_pred_ovr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgJPEKWNTHR",
        "outputId": "16d5cc54-88f3-4eda-8e9f-4ac55b8bda7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\"penalty\": (\"l1\", \"l2\", \"elasticnet\"), 'C': [1, 2, 10, 20, 30, 40]}\n",
        "classifier = LogisticRegression()\n",
        "clf = GridSearchCV(classifier , param_grid= params , cv=5 ,verbose=2)\n",
        "clf.fit(x_train,y_train)\n",
        "print(\"Gridsearchcv Best :\" ,clf.best_params_)\n",
        "print(\"Accuracy :\",clf.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOPRKOmxNYL-",
        "outputId": "c411bbf9-f4e3-4503-aff0-d12a185359bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.3s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.2s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.2s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "Gridsearchcv Best : {'C': 1, 'penalty': 'l2'}\n",
            "Accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "model_skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(LogisticRegression(max_iter=500), x, y, cv=model_skf)\n",
        "print(\"Stratified K-Fold Avg Accuracy:\", scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WBj00UzNcLY",
        "outputId": "6a12f8c9-070d-4c7c-ba4b-0b448347ac7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stratified K-Fold Avg Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "df = pd.read_csv(\"Car Sale.csv\")\n",
        "\n",
        "x = df.drop(\"Price ($)\" , axis=1)\n",
        "y = df[\"Price ($)\"]\n",
        "x_train_car , x_test_car , y_train_car , y_test_car = train_test_split(x,y, test_size=0.20,random_state=1)\n",
        "model_car = LogisticRegression(max_iter=500)\n",
        "model_car.fit(x_train_car,y_train_car)\n",
        "y_pred_car = model_car.predict(x_test_car)\n",
        "print(\"Accuracy :\",accuracy_score(y_train_car,y_pred_car))"
      ],
      "metadata": {
        "id": "MqLDbWfANh-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_clf = RandomizedSearchCV(classifier, param_distributions=params, n_iter=10, cv = 5)\n",
        "random_clf.fit(x_train,y_train)\n",
        "print(\"RandomizedSearchCV best:\", random_clf.best_params_)\n",
        "print(\"Accuracy:\", random_clf.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84yHbucSNlr8",
        "outputId": "0fcfa6fa-7a26-4c31-bf10-819d38225c86"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomizedSearchCV best: {'penalty': 'l2', 'C': 10}\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "model_ovo = OneVsOneClassifier(LogisticRegression(max_iter=500))\n",
        "model_ovo.fit(x_train, y_train)\n",
        "y_pred_ovo = model_ovo.predict(x_test)\n",
        "print(\"OvO Accuracy:\", accuracy_score(y_test,y_pred_ovo ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQXg-X0iNxwE",
        "outputId": "503f472c-16ce-45ad-9eb0-3185d9d82a84"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OvO Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(cm, cmap='Blues')\n",
        "plt.title('Confusion Matrix'); plt.colorbar()\n",
        "plt.xticks([0,1]); plt.yticks([0,1])\n",
        "plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j,i, cm[i,j], ha='center', va='center', color='white')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Z-tbBwu-N1W-",
        "outputId": "f34323d4-ebfc-4d4a-ec19-3f4814611570"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "p, r, f, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Precision:\", p)\n",
        "print(\"Recall:\", r)\n",
        "print(\"F1 Score:\", f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avw24K1CN7Xr",
        "outputId": "e807aded-b3f0-4a8c-e7fc-ae3d2529e666"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1 Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "y_imbal = np.hstack((np.zeros(len(y)-10), np.ones(10)))\n",
        "X_imbal = x[:len(y_imbal)]\n",
        "lr_w = LogisticRegression(class_weight='balanced', max_iter=500).fit(X_imbal, y_imbal)\n",
        "print(\"Imbalanced + class_weight Accuracy:\", accuracy_score(y_imbal, lr_w.predict(X_imbal)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4rQZYBJN_72",
        "outputId": "9e32a964-92a8-41af-8d1b-0bcc16b609d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imbalanced + class_weight Accuracy: 0.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(x_train)\n",
        "x_train, x_test = scaler.transform(x_train), scaler.transform(x_test)\n",
        "acc_no = accuracy_score(y_test, LogisticRegression(max_iter=500).fit(x_train, y_train).predict(x_test))\n",
        "acc_yes = accuracy_score(y_test, LogisticRegression(max_iter=500).fit(x_train, y_train).predict(x_test))\n",
        "print(\"without scaling:\", acc_no)\n",
        "print(\"With scaling:\", acc_yes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-svnSfqOMpY",
        "outputId": "420ada5e-47d1-474f-9fbe-6e2e088c4ace"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without scaling: 1.0\n",
            "With scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.metrics import roc_curve , auc\n",
        "\n",
        "y_pred_proba = model.predict_proba(x_test)[:,1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr , tpr)\n",
        "roc_auc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSyxzOEfOrdL",
        "outputId": "d9f91873-a9c3-4bf7-f1fc-ecc63787ba24"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "model_custom = LogisticRegression(C=0.5, max_iter=500)\n",
        "model_custom.fit(x_train,y_train)\n",
        "print(\"Accuracy Score :\", accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLLKKtuBOvVI",
        "outputId": "26cffa71-5ed1-464e-bbf6-3f559daed42c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "coefs = pd.Series(model_l2.coef_[0], index=data.feature_names)\n",
        "print(\"Top features:\\n\",coefs.sort_values(ascending=False).head())\n",
        "Top features:\n",
        " petal length (cm)    2.131055\n",
        "petal width (cm)     0.969290\n",
        "sepal length (cm)   -0.407355\n",
        "sepal width (cm)    -1.379103\n",
        "dtype: float64"
      ],
      "metadata": {
        "id": "bYSaXxOdO4vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "print(\"Cohen Kappa:\", cohen_kappa_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAeK4qHQPCio",
        "outputId": "b7fca472-a0fd-492a-c76c-36fc6482455d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen Kappa: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "prec, rec, thr = precision_recall_curve(y_test, y_pred_proba)\n",
        "plt.plot(rec, prec)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kywS9PAiPDX3",
        "outputId": "1e0c0368-6b76-4cf4-c090-6570f396fabd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "for sol in [\"liblinear\", \"saga\", \"lbfgs\"]:\n",
        "    model_solver = LogisticRegression(solver=sol , max_iter=500)\n",
        "    model_solver.fit(x_train,y_train)\n",
        "    y_pred_sol = model_solver.predict(x_test)\n",
        "    print(f\"solver {sol} acc : \" , accuracy_score(y_test,y_pred_sol))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aca4R6hPGvG",
        "outputId": "00ed740b-2b55-49c8-8dc3-52798349cfa3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solver liblinear acc :  1.0\n",
            "solver saga acc :  1.0\n",
            "solver lbfgs acc :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "print(\"MCC:\", matthews_corrcoef(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEqxgFLdPMBM",
        "outputId": "e871af2b-bec3-47e4-f3a1-2902c7a8531e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler().fit(x_train)\n",
        "x_train, x_test = scaler.transform(x_train), scaler.transform(x_test)\n",
        "acc_no = accuracy_score(y_test, LogisticRegression(max_iter=500).fit(x_train, y_train).predict(x_test))\n",
        "acc_yes = accuracy_score(y_test,LogisticRegression(max_iter=500).fit(x_train, y_train).predict(x_test))\n",
        "print(\"without scaling:\", acc_no)\n",
        "print(\"With scaling:\", acc_yes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxOiQ0UPQ5q",
        "outputId": "1e637059-fa5f-4ed8-edea-0d72d50efab1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "without scaling: 1.0\n",
            "With scaling: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params = {\"penalty\": (\"l1\", \"l2\", \"elasticnet\"), 'C': [1, 2, 10, 20, 30, 40]}\n",
        "classifier = LogisticRegression()\n",
        "clf = GridSearchCV(classifier , param_grid= params , cv=5 ,verbose=2)\n",
        "clf.fit(x_train,y_train)\n",
        "print(\"Gridsearchcv Best :\" ,clf.best_params_)\n",
        "print(\"Accuracy :\",clf.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkvJ6fmrPndZ",
        "outputId": "8df935ef-cacc-4cd8-addf-f919ef70d1a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=1, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ............................C=2, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=10, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.1s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=20, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=20, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=30, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=30, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...................................C=40, penalty=l2; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END ...........................C=40, penalty=elasticnet; total time=   0.0s\n",
            "Gridsearchcv Best : {'C': 1, 'penalty': 'l2'}\n",
            "Accuracy : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "joblib.dump(model_l2, 'lr_model.pkl')\n",
        "lr_loaded = joblib.load('lr_model.pkl')\n",
        "print(\"Loaded model accuracy:\", accuracy_score(y_test, lr_loaded.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00R0y6hXPiUA",
        "outputId": "6e55b5c1-d1f4-4cad-b764-992c6bee437f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CfIV7U0hP2bu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}